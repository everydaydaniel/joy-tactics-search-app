{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Audio from CSV and Transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript for 20240729_78_cream already exists. Skipping...\n",
      "Transcript for 20240725_reasons_to_live already exists. Skipping...\n",
      "Transcript for 20240722_77_what_is_the_wilderness_ft_joe_pera already exists. Skipping...\n",
      "Transcript for 20240718_the_notion_of_free_will already exists. Skipping...\n",
      "Transcript for 20240715_76_political_violnce already exists. Skipping...\n",
      "Transcript for 20240711_heated_interview already exists. Skipping...\n",
      "Transcript for 20240708_75_auras already exists. Skipping...\n",
      "Transcript for 20240704_manners already exists. Skipping...\n",
      "Transcript for 20240701_74_patriotism_jingoistically_speaking already exists. Skipping...\n",
      "Transcript for 20240627_heat already exists. Skipping...\n",
      "Transcript for 20240624_73_do_video_games_make_you_better_lovers already exists. Skipping...\n",
      "Transcript for 20240620_a_cure_for_thirst already exists. Skipping...\n",
      "Transcript for 20240617_72_lunch_pt_2 already exists. Skipping...\n",
      "Transcript for 20240613_the_notion_of_glass already exists. Skipping...\n",
      "Transcript for 20240610_71_occulary_sensory_deprivation_experiment already exists. Skipping...\n",
      "Transcript for 20240606_is_electricity_poison already exists. Skipping...\n",
      "Transcript for 20240603_70_high_school_ft_dan_licata already exists. Skipping...\n",
      "Transcript for 20240530_depletion already exists. Skipping...\n",
      "Transcript for 20240527_69_what_is_britain already exists. Skipping...\n",
      "Transcript for 20240523_shame already exists. Skipping...\n",
      "Transcript for 20240520_68_leveraging_dopamine_for_job_interviews already exists. Skipping...\n",
      "Transcript for 20240516_cute_animals already exists. Skipping...\n",
      "Transcript for 20240513_68_the_rich already exists. Skipping...\n",
      "Transcript for 20240509_first_attempt_at_talking_about_lunch already exists. Skipping...\n",
      "Transcript for 20240506_67_gambling_addict_confronted_by_two_bosnian_personal_trainers already exists. Skipping...\n",
      "Transcript for 20240502_driving_tour_of_brooklyn already exists. Skipping...\n",
      "Transcript for 20240429_66_driving_tour_of_manhattan already exists. Skipping...\n",
      "Transcript for 20240425_unknown already exists. Skipping...\n",
      "Transcript for 20240422_65_hospitality_ft_sarah_sherman already exists. Skipping...\n",
      "Transcript for 20240418_dreams already exists. Skipping...\n",
      "Transcript for 20240415_64_secrets already exists. Skipping...\n",
      "Transcript for 20240411_joy_tactics_meet_in_person_for_the_first_time already exists. Skipping...\n",
      "Transcript for 20240408_63_for_a_rainy_day already exists. Skipping...\n",
      "Transcript for 20240404_the_dark_side_of_hollywood already exists. Skipping...\n",
      "Transcript for 20240401_the_notion_of_fear already exists. Skipping...\n",
      "Transcript for 20240328_judging_a_book_by_its_cover_from_the_back already exists. Skipping...\n",
      "Transcript for 20240325_61_the_midwest already exists. Skipping...\n",
      "Transcript for 20240321_bad_day already exists. Skipping...\n",
      "Transcript for 20240318_59_spring_break_tales already exists. Skipping...\n",
      "Transcript for 20240314_midnight_snack already exists. Skipping...\n",
      "Transcript for 20240311_58_betrayal_the_story_of_the_cyberattack_from_within already exists. Skipping...\n",
      "Transcript for 20240307_the_immense_pressure_of_being_a_rising_podcaster already exists. Skipping...\n",
      "Transcript for 20240304_57_should_you_respect_old_people already exists. Skipping...\n",
      "Transcript for 20240229_relaxing_water_conversation already exists. Skipping...\n",
      "Transcript for 20240226_56_cars already exists. Skipping...\n",
      "Transcript for 20240222_emergency_survival_preparations already exists. Skipping...\n",
      "Transcript for 20240219_55_people_want_to_harm_us already exists. Skipping...\n",
      "Transcript for 20240215_how_to_cultivate_trustworthy_instincts already exists. Skipping...\n",
      "Transcript for 20240212_54_cheifs_win already exists. Skipping...\n",
      "Transcript for 20240208_guys_talking_grammys already exists. Skipping...\n",
      "Transcript for 20240205_30_year_olds_thinking_about_getting_in_shape already exists. Skipping...\n",
      "Transcript for 20240201_groceries already exists. Skipping...\n",
      "Transcript for 20240129_58_old_timey_baddies already exists. Skipping...\n",
      "Transcript for 20240125_1_year_anniversary_celebration already exists. Skipping...\n",
      "Transcript for 20240122_51_exclusive_ron_desantis_interview_without_ron_desantis already exists. Skipping...\n",
      "Transcript for 20240118_we_made_the_jacob_leiser_2023_list already exists. Skipping...\n",
      "Transcript for 20240115_50_wild_ass_celebrity_rumors already exists. Skipping...\n",
      "Transcript for 20240111_crazy_ass_golden_globes_recap already exists. Skipping...\n",
      "Transcript for 20240108_49_english_practice_conversation_about_travel already exists. Skipping...\n",
      "Transcript for 20240104_epstein_list_revealed_by_truth_artists already exists. Skipping...\n",
      "Transcript for 20240101_48_2024_stock_market_predictionsanalysis already exists. Skipping...\n",
      "Transcript for 20231228_the_nightcap_files already exists. Skipping...\n",
      "Transcript for 20231225_spend_christmas_with_three_struggling_comedians already exists. Skipping...\n",
      "Transcript for 20231221_christmas_pt_2___the_dark_side_of_christmas already exists. Skipping...\n",
      "Transcript for 20231218_46_christmast_joy_pt1 already exists. Skipping...\n",
      "Transcript for 20231214_mental__physical_toughness_bootcamp already exists. Skipping...\n",
      "Transcript for 20231211_the_first_episode_1182022 already exists. Skipping...\n",
      "Transcript for 20231207_boost_your_energy_in_3_days already exists. Skipping...\n",
      "Transcript for 20231204_45_tv_vs_movies_epic_debate already exists. Skipping...\n",
      "Transcript for 20231130_global_brand_marketplace_analysis already exists. Skipping...\n",
      "Transcript for 20231127_44_atheists_react_to_christmas_for_first_time already exists. Skipping...\n",
      "Transcript for 20231123_controversial_thanksgiving_debate already exists. Skipping...\n",
      "Transcript for 20231120_43_we_talk_to_a_rockstar_ft_hotline_tnts_will_anderson already exists. Skipping...\n",
      "Transcript for 20231116_complimenting_celebrities already exists. Skipping...\n",
      "Transcript for 20231113_42_rock_bottom already exists. Skipping...\n",
      "Transcript for 20231109_richuals already exists. Skipping...\n",
      "Transcript for 20231106_41_morning_meditations already exists. Skipping...\n",
      "Transcript for 20231102_2023_is_officially_over_its_time_to_get_real already exists. Skipping...\n",
      "Transcript for 20231030_40_joy_ween already exists. Skipping...\n",
      "Transcript for 20231026_bonus_how_to_roll_with_punches already exists. Skipping...\n",
      "Transcript for 20231023_39_regarding_the_current_state_of_the_music_industry already exists. Skipping...\n",
      "Transcript for 20231019_joy_tactics_live_in_ny already exists. Skipping...\n",
      "Transcript for 20231016_38_being_late_to_stuff already exists. Skipping...\n",
      "Transcript for 20231012_coming_out_of_our_cocoons already exists. Skipping...\n",
      "Transcript for 20231005_regarding_the_current_state_of_comedy already exists. Skipping...\n",
      "Transcript for 20231002_36_recovery already exists. Skipping...\n",
      "Transcript for 20230928_intense_claudia_odoherty_interview already exists. Skipping...\n",
      "Transcript for 20230925_35_alienz_ft_conner_omalley already exists. Skipping...\n",
      "Transcript for 20230921_fashion_ft_megan_stalter already exists. Skipping...\n",
      "Transcript for 20230918_joy_tactics_live_in_la_ft_john_early_and_sarah_sherman already exists. Skipping...\n",
      "Transcript for 20230914_la_vs_nyc already exists. Skipping...\n",
      "Transcript for 20230911_33_taking_risks_can_destroy_you already exists. Skipping...\n",
      "Transcript for 20230907_crazy_ass_celebrity_gossip__ama already exists. Skipping...\n",
      "Transcript for 20230904_33_health already exists. Skipping...\n",
      "Transcript for 20230831_apologizing already exists. Skipping...\n",
      "Transcript for 20230828_32_love already exists. Skipping...\n",
      "Transcript for 20230824_emergency_broadcast_2 already exists. Skipping...\n",
      "Transcript for 20230821_31_the_perfect_podcast_for_hospitals already exists. Skipping...\n",
      "Transcript for 20230817_addressing_the_reddit_drama already exists. Skipping...\n",
      "Transcript for 20230814_30_food already exists. Skipping...\n",
      "Transcript for 20230810_addressing_the_eric_rahill_drama already exists. Skipping...\n",
      "Transcript for 20230807_29_emergency_broadcast already exists. Skipping...\n",
      "Transcript for 20230803_classic_rock_ft_rachel_kaly already exists. Skipping...\n",
      "Transcript for 20230731_28_the_kid_friendly_episode already exists. Skipping...\n",
      "Transcript for 20230727_nature_walk_turns_into_medical_emergency already exists. Skipping...\n",
      "Transcript for 20230724_27_control already exists. Skipping...\n",
      "Transcript for 20230720_do_vacations_make_you_happy already exists. Skipping...\n",
      "Transcript for 20230717_26_erics_bday already exists. Skipping...\n",
      "Transcript for 20230713_winston_churchill_biography already exists. Skipping...\n",
      "Transcript for 20230710_25_videogames already exists. Skipping...\n",
      "Transcript for 20230706_simulated_early_human_discussion already exists. Skipping...\n",
      "Transcript for 20230703_24_alter_egos already exists. Skipping...\n",
      "Transcript for 20230629_our_new_summer_bodies already exists. Skipping...\n",
      "Transcript for 20230626_23_interview_with_3_featured_extras_from_the_bear already exists. Skipping...\n",
      "Transcript for 20230622_choose_happiness already exists. Skipping...\n",
      "Transcript for 20230619_22_murder_is_weird already exists. Skipping...\n",
      "Transcript for 20230615__watch_out_for_these_red_flags_in_your_romantic_and_professional_relationships already exists. Skipping...\n",
      "Transcript for 20230612_21_alcohol_is_ruining_the_world already exists. Skipping...\n",
      "Transcript for 20230608_current_events_pt_2 already exists. Skipping...\n",
      "Transcript for 20230605_20_current_events already exists. Skipping...\n",
      "Transcript for 20230601_unsolved_mysteries_hour already exists. Skipping...\n",
      "Transcript for 20230529_19_explaining_what_is_outer_space already exists. Skipping...\n",
      "Transcript for 20230525_the_cinema already exists. Skipping...\n",
      "Transcript for 20230522_18_david__goliath__video already exists. Skipping...\n",
      "Transcript for 20230518_all_w33ded_out already exists. Skipping...\n",
      "Transcript for 20230515_17_charlies_male_angels already exists. Skipping...\n",
      "Transcript for 20230511_tough_times_rawr already exists. Skipping...\n",
      "Transcript for 20230508_16_respect_to_the_monarchy already exists. Skipping...\n",
      "Transcript for 20230504_massive_personal_growth_accelerator_e_course already exists. Skipping...\n",
      "Transcript for 20230501_15_jerry_springers_legacy already exists. Skipping...\n",
      "Transcript for 20230427_mcdonalds_mukbang already exists. Skipping...\n",
      "Transcript for 20230424_14_healing_frequencies already exists. Skipping...\n",
      "Transcript for 20230420_420_verbal_narcan already exists. Skipping...\n",
      "Transcript for 20230417_13_brace_belden_from_true_anon_explains_fruit already exists. Skipping...\n",
      "Transcript for 20230413_miss_swift already exists. Skipping...\n",
      "Transcript for 20230410_12_we_are_begging_to_be_sued already exists. Skipping...\n",
      "Transcript for 20230406_erics_computer_is_trash already exists. Skipping...\n",
      "Transcript for 20230403_11_hilariously_answering_your_effed_up_questions already exists. Skipping...\n",
      "Transcript for 20230330_irl_rick_roll already exists. Skipping...\n",
      "Transcript for 20230327_10_good_morning already exists. Skipping...\n",
      "Transcript for 20230323_congratulations_to_selena_gomez already exists. Skipping...\n",
      "Transcript for 20230320_9_demons__kindess already exists. Skipping...\n",
      "Transcript for 20230316_brain__reality already exists. Skipping...\n",
      "Transcript for 20230313_8_a_grave_warning_to_corporate_america already exists. Skipping...\n",
      "Transcript for 20230309_discussing_our_personal_upbringings_and_celebrities already exists. Skipping...\n",
      "Transcript for 20230306_7_love_me_thru_the_phone already exists. Skipping...\n",
      "Transcript for 20230302_dark_duolingo already exists. Skipping...\n",
      "Transcript for 20230227_6_snls_sarah_sherman_retires_from_comedy_ft_sarah_sherman already exists. Skipping...\n",
      "Audio for 20230223_psyop_type_sht already exists. Skipping download...\n",
      "Audio for 20230220_5_the_drunk_files already exists. Skipping download...\n",
      "Audio for 20230216_alien_swag already exists. Skipping download...\n",
      "Audio for 20230213_4_mylke__coogies already exists. Skipping download...\n",
      "Audio for 20230209_utopian_possibilities already exists. Skipping download...\n",
      "Audio for 20230206_3_metaphorical_knowledge already exists. Skipping download...\n",
      "Audio for 20230202_religious_discussion_no_holds_barred already exists. Skipping download...\n",
      "Audio for 20230130_2_royalty_free_acappella_verses already exists. Skipping download...\n",
      "Audio for 20230126_get_to_know_us_more_better already exists. Skipping download...\n",
      "Audio for 20230123_1_crazy_ass_coachella_lineup_reaction already exists. Skipping download...\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "import yt_dlp\n",
    "from pydub import AudioSegment\n",
    "import whisper\n",
    "from datetime import timedelta, datetime\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"FP16 is not supported on CPU; using FP32 instead\")\n",
    "\n",
    "def download_audio(url, filename):\n",
    "    temp_filename = filename.replace('.mp3', '') + \".temp.mp3\"\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'mp3',\n",
    "            'preferredquality': '192',\n",
    "        }],\n",
    "        'outtmpl': temp_filename.replace('.mp3', ''),  # Use the specified temp filename without .mp3\n",
    "        'ffmpeg_location': '/usr/local/bin',  # Specify the path to ffmpeg if needed\n",
    "    }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([url])\n",
    "\n",
    "    # Rename temp file to final filename\n",
    "    if os.path.exists(temp_filename):\n",
    "        os.rename(temp_filename, filename)\n",
    "\n",
    "    # Get the duration of the audio file\n",
    "    audio = AudioSegment.from_file(filename, format=\"mp3\")\n",
    "    duration_minutes = len(audio) / 60000  # Convert milliseconds to minutes\n",
    "    return duration_minutes\n",
    "\n",
    "def split_audio(file_path, chunk_length_ms=300000):  # 5 minutes = 300,000 ms\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    audio = AudioSegment.from_mp3(file_path)\n",
    "    chunks = [audio[i:i+chunk_length_ms] for i in range(0, len(audio), chunk_length_ms)]\n",
    "    chunk_files = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_filename = os.path.join('data/audio/split', f\"{os.path.basename(file_path)[:-4]}_chunk{i}.mp3\")\n",
    "        chunk.export(chunk_filename, format=\"mp3\")\n",
    "        chunk_files.append(chunk_filename)\n",
    "    return chunk_files\n",
    "\n",
    "def clean_title(title):\n",
    "    # Remove brackets and anything inside them\n",
    "    title = re.sub(r'\\[.*?\\]', '', title)\n",
    "    # Remove anything after and including the pipe symbol\n",
    "    title = re.split(r'\\|', title)[0]\n",
    "    # Replace spaces and hyphens with underscores\n",
    "    title = title.replace(' ', '_').replace('-', '_')\n",
    "    # Remove any non-alphanumeric characters except underscores\n",
    "    title = re.sub(r'[^a-zA-Z0-9_]', '', title)\n",
    "    # Remove trailing underscores\n",
    "    title = title.rstrip('_')\n",
    "    return title.lower()\n",
    "\n",
    "def format_timestamp(seconds):\n",
    "    return str(timedelta(seconds=seconds))\n",
    "\n",
    "def transcribe_audio(file_path):\n",
    "    model = whisper.load_model(\"base\")\n",
    "    result = model.transcribe(file_path)\n",
    "    segments = result['segments']\n",
    "\n",
    "    transcript = \"\"\n",
    "    for segment in segments:\n",
    "        start_time = format_timestamp(segment['start'])\n",
    "        end_time = format_timestamp(segment['end'])\n",
    "        text = segment['text']\n",
    "        transcript += f\"{start_time} - {end_time}::{text}\\n\"\n",
    "\n",
    "    return transcript\n",
    "\n",
    "def save_chunk_transcript(transcript, chunk_filename):\n",
    "    chunk_transcript_path = os.path.join('data/transcripts/chunks', f\"{os.path.basename(chunk_filename)[:-4]}.txt\")\n",
    "    with open(chunk_transcript_path, \"w\") as f:\n",
    "        f.write(transcript)\n",
    "    return chunk_transcript_path\n",
    "\n",
    "def transcribe_chunks(chunk_files, formatted_title):\n",
    "    full_transcript = \"\"\n",
    "    for i, chunk_file in enumerate(sorted(chunk_files, key=lambda x: int(re.search(r'_chunk(\\d+)', x).group(1)))):\n",
    "        chunk_transcript_path = os.path.join('data/transcripts/chunks', f\"{os.path.basename(chunk_file)[:-4]}.txt\")\n",
    "        if os.path.exists(chunk_transcript_path):\n",
    "            print(f\"Transcript for {chunk_file} already exists. Skipping transcription...\")\n",
    "            with open(chunk_transcript_path, \"r\") as f:\n",
    "                transcript = f.read()\n",
    "        else:\n",
    "            transcript = transcribe_audio(chunk_file)\n",
    "            save_chunk_transcript(transcript, chunk_file)\n",
    "        full_transcript += adjust_timestamps(transcript, i * 300) + \"\\n\"\n",
    "    return full_transcript\n",
    "\n",
    "def adjust_timestamps(transcript, offset_seconds):\n",
    "    adjusted_transcript = \"\"\n",
    "    for line in transcript.split(\"\\n\"):\n",
    "        if line.strip():\n",
    "            try:\n",
    "                time_range, text = line.split(\"::\", 1)\n",
    "                start_time, end_time = time_range.split(\" - \")\n",
    "                adjusted_start_time = format_timestamp(convert_timestamp(start_time).total_seconds() + offset_seconds)\n",
    "                adjusted_end_time = format_timestamp(convert_timestamp(end_time).total_seconds() + offset_seconds)\n",
    "                adjusted_transcript += f\"{adjusted_start_time} - {adjusted_end_time}::{text.strip()}\\n\"\n",
    "            except ValueError as e:\n",
    "                print(f\"Skipping line due to unexpected format: {line}. Error: {e}\")  # Debug print\n",
    "                adjusted_transcript += line + \"\\n\"\n",
    "    return adjusted_transcript\n",
    "\n",
    "def convert_timestamp(timestamp):\n",
    "    parts = timestamp.split(\":\")\n",
    "    if len(parts) == 2:  # MM:SS format\n",
    "        m, s = map(float, parts)\n",
    "        h = 0\n",
    "    elif len(parts) == 3:  # HH:MM:SS format\n",
    "        h, m, s = map(float, parts)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid timestamp format: {timestamp}\")\n",
    "    return timedelta(hours=h, minutes=m, seconds=s)\n",
    "\n",
    "def extract_date_from_posted_date(posted_date):\n",
    "    # Extract the date part from 'YYYY-MM-DD HH:MM:SS' format and convert it to 'YYYYMMDD'\n",
    "    return datetime.strptime(posted_date.split()[0], '%Y-%m-%d').strftime('%Y%m%d')\n",
    "\n",
    "def update_csv_with_duration(csv_path, formatted_title, duration):\n",
    "    temp_file = csv_path + '.tmp'\n",
    "    with open(csv_path, 'r', newline='') as csvfile, open(temp_file, 'w', newline='') as tempcsv:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        fieldnames = reader.fieldnames if 'duration' in reader.fieldnames else reader.fieldnames + ['duration']\n",
    "        writer = csv.DictWriter(tempcsv, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        for row in reader:\n",
    "            row_title = f\"{extract_date_from_posted_date(row['date_posted'])}_{clean_title(row['formatted_title'])}\"\n",
    "            if row_title == formatted_title:\n",
    "                row['duration'] = duration\n",
    "            writer.writerow(row)\n",
    "    os.replace(temp_file, csv_path)\n",
    "\n",
    "def process_episode(url, episode_title, posted_date, csv_path):\n",
    "    # Clean the episode title to create a safe filename\n",
    "    formatted_title = clean_title(episode_title)\n",
    "    date_prefix = extract_date_from_posted_date(posted_date)\n",
    "    full_title = f\"{date_prefix}_{formatted_title}\"\n",
    "    filename = os.path.join('data/audio/full', f\"{full_title}.mp3\")\n",
    "    transcript_filename = os.path.join('data/transcripts/full', f\"{full_title}.txt\")\n",
    "    \n",
    "    # Skip if transcript already exists\n",
    "    if os.path.exists(transcript_filename):\n",
    "        print(f\"Transcript for {full_title} already exists. Skipping...\")\n",
    "        return\n",
    "    \n",
    "    # Check if audio file already exists\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"Audio for {full_title} already exists. Skipping download...\")\n",
    "        audio = AudioSegment.from_file(filename, format=\"mp3\")\n",
    "        duration_minutes = len(audio) / 60000\n",
    "    else:\n",
    "        # Download the audio and get duration\n",
    "        duration_minutes = download_audio(url, filename)\n",
    "        update_csv_with_duration(csv_path, full_title, duration_minutes)\n",
    "    \n",
    "    # Calculate expected number of chunks\n",
    "    expected_chunks = (int(duration_minutes) // 5) + 1\n",
    "\n",
    "    # Check if split files already exist\n",
    "    expected_chunk_filename = os.path.join('data/audio/split', f\"{full_title}_chunk0.mp3\")\n",
    "    chunk_files = []\n",
    "    if os.path.exists(expected_chunk_filename):\n",
    "        print(f\"Split files for {full_title} already exist. Skipping splitting...\")\n",
    "        chunk_files = sorted([os.path.join('data/audio/split', f) for f in os.listdir('data/audio/split') if f.startswith(f\"{full_title}\") and f.endswith('.mp3')], key=lambda x: int(re.search(r'_chunk(\\d+)', x).group(1)))\n",
    "        if len(chunk_files) != expected_chunks:\n",
    "            print(f\"Incorrect number of chunk files for {full_title}. Expected {expected_chunks}, but found {len(chunk_files)}.\")\n",
    "            chunk_files = split_audio(filename)\n",
    "    else:\n",
    "        # Split the audio into chunks\n",
    "        chunk_files = split_audio(filename)\n",
    "    \n",
    "    # Transcribe the chunks\n",
    "    full_transcript = transcribe_chunks(chunk_files, full_title)\n",
    "    \n",
    "    # Save the full transcript\n",
    "    with open(transcript_filename, \"w\") as f:\n",
    "        f.write(full_transcript)\n",
    "    \n",
    "    return chunk_files\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs('data/audio/full', exist_ok=True)\n",
    "    os.makedirs('data/audio/split', exist_ok=True)\n",
    "    os.makedirs('data/transcripts', exist_ok=True)\n",
    "    os.makedirs('data/transcripts/chunks', exist_ok=True)\n",
    "    \n",
    "    # Read the CSV file and filter rows with non-empty youtube_url\n",
    "    csv_path = 'data/episodes.csv'\n",
    "    with open(csv_path, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        filtered_rows = [row for row in reader if row['youtube_url']]\n",
    "        \n",
    "    for i, row in enumerate(filtered_rows):\n",
    "        episode_url = row['youtube_url']\n",
    "        episode_title = row['formatted_title']\n",
    "        posted_date = row['date_posted']\n",
    "        process_episode(episode_url, episode_title, posted_date, csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
